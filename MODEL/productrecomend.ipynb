{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9744531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'user_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_243896\\2214522755.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df[df[\u001b[33m\"stock\"\u001b[39m] > \u001b[32m0\u001b[39m][\u001b[33m\"productId\"\u001b[39m].drop_duplicates().head(\u001b[32m10\u001b[39m).tolist()\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m __name__ == \u001b[33m\"__main__\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     full_df = preprocess_data()\n\u001b[32m    165\u001b[39m     full_df[\u001b[33m\"combined_text\"\u001b[39m] = create_text_features(full_df)\n\u001b[32m    166\u001b[39m \n\u001b[32m    167\u001b[39m     unique_products_df = full_df.drop_duplicates(subset=\"productId\").reset_index(\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_243896\\2214522755.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     57\u001b[39m     cart_flat = flatten_user_products(users, \u001b[33m'cartProducts'\u001b[39m)\n\u001b[32m     58\u001b[39m     wish_flat = flatten_user_products(users, \u001b[33m'wishListProducts'\u001b[39m)\n\u001b[32m     59\u001b[39m     user_products_df = pd.concat([cart_flat, wish_flat], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     60\u001b[39m     interactions.rename(columns={\u001b[33m'userId'\u001b[39m: \u001b[33m'user_id'\u001b[39m}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     merged_interactions = pd.merge(user_products_df, interactions, how=\u001b[33m'outer'\u001b[39m, on=[\u001b[33m'user_id'\u001b[39m, \u001b[33m'productId'\u001b[39m])\n\u001b[32m     62\u001b[39m     merged_interactions[\u001b[33m'type'\u001b[39m] = merged_interactions[\u001b[33m'type_x'\u001b[39m].combine_first(merged_interactions[\u001b[33m'type_y'\u001b[39m])\n\u001b[32m     63\u001b[39m     merged_interactions.drop(columns=[\u001b[33m'type_x'\u001b[39m, \u001b[33m'type_y'\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     64\u001b[39m     merged_interactions[\u001b[33m'type'\u001b[39m] = merged_interactions[\u001b[33m'type'\u001b[39m].fillna(\u001b[33m'view'\u001b[39m)\n",
      "\u001b[32mc:\\Users\\nithy\\miniconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\nithy\\miniconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\nithy\\miniconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1293\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1296\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1298\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1299\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1300\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32mc:\\Users\\nithy\\miniconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'user_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def fetch_realtime_data():\n",
    "    MONGO_URI = \"mongodb://localhost:27017\"\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[\"My-Shop\"]\n",
    "    return (\n",
    "        pd.DataFrame(list(db.users.find())),\n",
    "        pd.DataFrame(list(db.interactions.find())),\n",
    "        pd.DataFrame(list(db.products.find()))\n",
    "    )\n",
    "\n",
    "def convert_mongo_types(df, id_cols=[], date_cols=[]):\n",
    "    for col in id_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: str(x) if pd.notnull(x) else x)\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "def preprocess_data():\n",
    "    users, interactions, products = fetch_realtime_data()\n",
    "    \n",
    "    users = convert_mongo_types(users, id_cols=['_id'], date_cols=[])\n",
    "    interactions = convert_mongo_types(interactions, id_cols=['productId'], date_cols=['interactionDate'])\n",
    "    products = convert_mongo_types(products, id_cols=['_id'], date_cols=[])\n",
    "    products.drop(columns=['dimensions','reviews','minimumOrderQuantity','image','warrantyInformation','weight','availabilityStatus'], inplace=True, errors='ignore')\n",
    "    users.drop(columns=['image','role','password','email','phone','__v'], inplace=True, errors='ignore')\n",
    "\n",
    "    def flatten_user_products(users_df, list_type='cartProducts'):\n",
    "        flattened = []\n",
    "        for user in users_df.to_dict('records'):\n",
    "            product_list = user.get(list_type, [])\n",
    "            if not isinstance(product_list, list):\n",
    "                continue\n",
    "            for product in product_list:\n",
    "                if not isinstance(product, dict):\n",
    "                    continue\n",
    "                flattened.append({\n",
    "                    'user_id': user.get('_id'),\n",
    "                    'username': user.get('username'),\n",
    "                    'productId': product.get('_id'),\n",
    "                    'quantity': product.get('quantity'),\n",
    "                    'size': product.get('size'),\n",
    "                    'type': 'cart' if list_type == 'cartProducts' else 'wishlist'\n",
    "                })\n",
    "        return pd.DataFrame(flattened)\n",
    "    \n",
    "  \n",
    "    \n",
    "    cart_flat = flatten_user_products(users, 'cartProducts')\n",
    "    wish_flat = flatten_user_products(users, 'wishListProducts')\n",
    "    user_products_df = pd.concat([cart_flat, wish_flat], ignore_index=True)\n",
    "    interactions.rename(columns={'userId': 'user_id'}, inplace=True)\n",
    "    merged_interactions = pd.merge(user_products_df, interactions, how='outer', on=['user_id', 'productId'])\n",
    "    merged_interactions['type'] = merged_interactions['type_x'].combine_first(merged_interactions['type_y'])\n",
    "    merged_interactions.drop(columns=['type_x', 'type_y'], inplace=True)\n",
    "    merged_interactions['type'] = merged_interactions['type'].fillna('view')\n",
    "    merged_interactions['productId'] = merged_interactions['productId'].astype(str)\n",
    "    products['_id'] = products['_id'].astype(str)\n",
    "    filtered_products = products[products['_id'].isin(merged_interactions['productId'])].copy()\n",
    "    final_df = pd.merge(merged_interactions, filtered_products, how='left', left_on='productId', right_on='_id')\n",
    "    final_df.drop(columns=[col for col in ['_id', '_id_x', '_id_y'] if col in final_df.columns], inplace=True)\n",
    "    final_df.sort_values(by=['user_id', 'productId'], inplace=True)\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "    final_df['combined_text'] = create_text_features(final_df)\n",
    "    final_df.fillna({'username': 'user', 'size': 'M', 'quantity': 1, 'interactionDate': pd.Timestamp('2024-10-20 15:04:49')}, inplace=True)\n",
    "    print(cart_flat)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "def create_text_features(df):\n",
    "    text_components = []\n",
    "    for col in [\n",
    "        \"subcategory\",\n",
    "        \"tags\",\n",
    "        \"brand\",\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        \"sku\",\n",
    "        \"category\",\n",
    "    ]:\n",
    "        if col == \"tags\":\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: \", \".join(map(str, x)) if isinstance(x, list) else \"\"\n",
    "            )\n",
    "        df[col] = df[col].fillna(\"\").astype(str).str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "        text_components.append(df[col])\n",
    "    return text_components[0].str.cat(text_components[1:], sep=\" \")\n",
    "\n",
    "\n",
    "def safe_feature_scaling(df, features, prefix=\"scaled\"):\n",
    "    valid_features = [f for f in features if f in df.columns]\n",
    "    df_filled = df[valid_features].copy()\n",
    "    for col in valid_features:\n",
    "        df_filled[col] = pd.to_numeric(df_filled[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_array = scaler.fit_transform(df_filled)\n",
    "    scaled_df = pd.DataFrame(scaled_array, columns=valid_features)\n",
    "\n",
    "    # Apply custom weights for tuning\n",
    "    weight_map = {\"price\": 0.5, \"discountPercentage\": 2.0, \"rating\": 3.0, \"stock\": 1.0}\n",
    "\n",
    "    for col in scaled_df.columns:\n",
    "        if col in weight_map:\n",
    "            scaled_df[col] *= weight_map[col]\n",
    "\n",
    "    return scaled_df.add_prefix(f\"{prefix}_\")\n",
    "\n",
    "\n",
    "def realtime_similarity_engine(df, text_col=\"combined_text\", num_cols=None):\n",
    "    tfidf = TfidfVectorizer(\n",
    "        stop_words=\"english\", max_features=8000, ngram_range=(1, 2), min_df=2\n",
    "    )\n",
    "    tfidf_matrix = tfidf.fit_transform(df[text_col])\n",
    "\n",
    "    num_scaled = safe_feature_scaling(df, num_cols)\n",
    "    text_weight = 1.2\n",
    "    combined_features = hstack([tfidf_matrix * text_weight, num_scaled]).tocsr()\n",
    "\n",
    "    n_neighbors = min(30, len(df) - 1)\n",
    "    nn = NearestNeighbors(\n",
    "        n_neighbors=n_neighbors + 1, metric=\"cosine\", algorithm=\"brute\"\n",
    "    )\n",
    "    nn.fit(combined_features)\n",
    "\n",
    "    return nn, combined_features\n",
    "\n",
    "\n",
    "def realtime_recommendations(product_id, model, features_matrix, df, top_n=10):\n",
    "    try:\n",
    "        product_idx = df[df[\"productId\"] == product_id].index[0]\n",
    "\n",
    "        max_neighbors = min(\n",
    "            top_n + 20, features_matrix.shape[0]\n",
    "        )  # Safely limit n_neighbors\n",
    "        distances, indices = model.kneighbors(\n",
    "            features_matrix[product_idx], n_neighbors=max_neighbors\n",
    "        )\n",
    "\n",
    "        results = df.iloc[indices[0]].copy()\n",
    "        results = results[results[\"productId\"] != product_id]\n",
    "        results = results.drop_duplicates(subset=\"productId\")\n",
    "        results = results[results[\"stock\"] > 0].head(top_n)\n",
    "\n",
    "        return results[\"productId\"].tolist()\n",
    "    except (IndexError, KeyError):\n",
    "        return get_fallback_recommendations(df)\n",
    "\n",
    "\n",
    "def get_fallback_recommendations(df):\n",
    "    return df[df[\"stock\"] > 0][\"productId\"].drop_duplicates().head(10).tolist()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    full_df = preprocess_data()\n",
    "    full_df[\"combined_text\"] = create_text_features(full_df)\n",
    "\n",
    "    unique_products_df = full_df.drop_duplicates(subset=\"productId\").reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    print(\"Final Data Shape:\", unique_products_df.shape)\n",
    "    print(\"Unique Products:\", unique_products_df[\"productId\"].nunique())\n",
    "\n",
    "    model, feature_matrix = realtime_similarity_engine(\n",
    "        unique_products_df, num_cols=[\"price\", \"discountPercentage\", \"rating\", \"stock\"]\n",
    "    )\n",
    "\n",
    "    sample_id = unique_products_df[\"productId\"].iloc[11\n",
    "    ]\n",
    "    print(\"Sample Product:\", unique_products_df.loc[11\n",
    "    , \"title\"])\n",
    "\n",
    "    recommendations = realtime_recommendations(\n",
    "        sample_id, model, feature_matrix, unique_products_df\n",
    "    )\n",
    "    print(\"Recommended product IDs:\", sample_id, recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
